import os
import torch
import random
from datasets import Dataset, load_from_disk
from transformers import (
    AutoTokenizer, Trainer, TrainingArguments, AutoModelForCausalLM,
    DataCollatorForSeq2Seq, BitsAndBytesConfig, AutoConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import chardet

os.environ['CUDA_LAUNCH_BLOCKING'] = "1"

#  Model and Path Configuration
MODEL_NAME = "microsoft/Phi-3-mini-4k-instruct"
OUTPUT_DIR = "./fine_tuned_phi3_new"
DATA_FOLDER = "./StoriesAll"
CACHE_PATH = "./cached_dataset_phi3_new"

#  Training Hyperparameters for Phi-3 (Adjusted for RTX 3070)
MAX_LENGTH_C = 2048  # Token length per chunk
BATCH_SIZE = 2  # Keep small due to VRAM limitations
GRAD_ACCUM_STEPS = 8  # Effective batch size remains high
LEARNING_RATE = 1.5e-4
EPOCHS = 3

#  4-bit Quantization for Memory Optimization
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

#  Load Model with QLoRA
base_model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    device_map="auto",
    quantization_config=bnb_config,
    use_cache=False,
    return_dict=True
)

print(base_model)
for name, module in base_model.named_modules():
    print(name)

#  Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token

lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # Adjust module names!
    lora_dropout=0.1,
    bias="lora_only",  # Enable bias
    task_type="CAUSAL_LM",
)

#  Prepare LoRA
base_model = prepare_model_for_kbit_training(base_model)
model = get_peft_model(base_model, lora_config)
print("LoRA Adapters Added")

#  Load Stories (with Filtering for Specific Terms)
def detect_and_convert_to_utf8(file_path):
    """Detects encoding and converts file to UTF-8."""
    with open(file_path, 'rb') as f:
        raw_data = f.read()
        encoding = chardet.detect(raw_data)['encoding']
    with open(file_path, 'r', encoding=encoding) as f:
        return f.read()

def load_stories(folder_path, num_samples=700):
    """Loads stories, filters certain content, and selects a random sample."""
    stories = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.endswith('.txt'):
                file_path = os.path.join(root, file)
                try:
                    text = open(file_path, 'r').readline()
                    stories.append(detect_and_convert_to_utf8(file_path))
                except Exception as e:
                    print(f"Error with file {file_path}: {e}")

    random.shuffle(stories)
    return stories[:num_samples]

#  Chunking Stories with Overlapping
def chunk_stories(stories, max_length_c, overlap=0.5):
    """Splits stories into overlapping segments."""
    prompt_response_pairs = []
    stride = int(max_length_c * (1 - overlap))

    for story in stories:
        story = "Complete this story: " + story
        for i in range(0, len(story) - max_length_c + 1, stride):
            prompt = story[i:i + max_length_c]
            response = story[i + max_length_c:i + max_length_c * 2]
            prompt_response_pairs.append({"prompt": prompt, "response": response})

        if len(story) % stride != 0:
            prompt = story[-max_length_c:]
            response = story[-max_length_c * 2:-max_length_c]
            prompt_response_pairs.append({"prompt": prompt, "response": response})

    return prompt_response_pairs

#  Tokenization
def tokenize_prompt_response_pairs(examples):
    """Tokenizes prompt-response pairs."""
    tokenized = tokenizer(
        examples["prompt"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LENGTH_C,
    )
    tokenized["labels"] = tokenizer(
        examples["response"],
        truncation=True,
        padding="max_length",
        max_length=MAX_LENGTH_C
    )["input_ids"]
    return tokenized

# Load or Create Dataset
if os.path.exists(CACHE_PATH):
    print("Loading dataset from cache...")
    tokenized_datasets = load_from_disk(CACHE_PATH)
else:
    print("Loading and processing stories...")
    all_stories = load_stories(DATA_FOLDER)
    prompt_response_pairs = chunk_stories(all_stories, MAX_LENGTH_C)

    prompt_response_dataset = Dataset.from_dict({
        "prompt": [pair["prompt"] for pair in prompt_response_pairs],
        "response": [pair["response"] for pair in prompt_response_pairs]
    })

    tokenized_datasets = prompt_response_dataset.map(tokenize_prompt_response_pairs, batched=True)
    tokenized_datasets.save_to_disk(CACHE_PATH)

split_datasets = tokenized_datasets.train_test_split(test_size=0.1)
train_dataset = split_datasets["train"]
eval_dataset = split_datasets["test"]

# Training Arguments
training_args = TrainingArguments(
    output_dir="./results_phi3",
    evaluation_strategy="epoch",
    learning_rate=LEARNING_RATE,
    per_device_train_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM_STEPS,
    num_train_epochs=EPOCHS,
    save_strategy="steps",
    save_steps=500,
    fp16=True,
    logging_dir="./logs_phi3",
    logging_steps=25,
    save_total_limit=2,
)

# Data Collator
data_collator = DataCollatorForSeq2Seq(
    tokenizer,
    model=model,
    label_pad_token_id=-100,
    padding=True,
    max_length=MAX_LENGTH_C,
    return_tensors="pt",
)

# Set Up Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
)

# Start Training
print("Starting training...")
checkpoint_path = "./results_phi3/checkpoint-500"
if os.path.exists(checkpoint_path):
    print(f"Resuming from checkpoint: {checkpoint_path}")
    trainer.train(resume_from_checkpoint=checkpoint_path)
else:
    trainer.train()

# Save Model
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("Model saved!")
